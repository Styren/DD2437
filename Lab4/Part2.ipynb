{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boiler Plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bin_train = np.genfromtxt('data/bindigit_trn.csv',delimiter=',',)\n",
    "target_train = np.genfromtxt('data/targetdigit_trn.csv',delimiter=',',)\n",
    "bin_test = np.genfromtxt('data/bindigit_tst.csv',delimiter=',',)\n",
    "target_test = np.genfromtxt('data/targetdigit_tst.csv',delimiter=',',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def report_DBN(rbms = [150,100,50], iterations = 50, batch_size = 64, learning_rate = 0.01, verbose = True):\n",
    "    #Initialize pipe\n",
    "    networks = []\n",
    "    count = 1\n",
    "    for hidden_dim in rbms:\n",
    "        rbm = BernoulliRBM(n_components = hidden_dim, batch_size = batch_size, learning_rate = learning_rate,\n",
    "                      n_iter = iterations, verbose = verbose, random_state = 0)\n",
    "        networks.append(('rbm{}'.format(count),rbm))\n",
    "        count += 1\n",
    "\n",
    "    networks.append(('mlp',MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam',alpha=0.01)))\n",
    "    model = Pipeline(networks)\n",
    "    \n",
    "    #Train model\n",
    "    model.fit(bin_train, target_train)\n",
    "\n",
    "    #Report results\n",
    "    print(metrics.classification_report(\n",
    "        target_test,\n",
    "        model.predict(bin_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.98      0.97       196\n",
      "        1.0       0.96      0.97      0.96       227\n",
      "        2.0       0.95      0.92      0.93       206\n",
      "        3.0       0.95      0.92      0.94       202\n",
      "        4.0       0.95      0.96      0.96       196\n",
      "        5.0       0.91      0.91      0.91       182\n",
      "        6.0       0.94      0.96      0.95       191\n",
      "        7.0       0.95      0.92      0.94       205\n",
      "        8.0       0.93      0.94      0.94       194\n",
      "        9.0       0.93      0.96      0.94       201\n",
      "\n",
      "avg / total       0.95      0.95      0.95      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_DBN(rbms = [], iterations = 100, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_stacked_autoencoders(autoencoders, iterations = 50, batch_size=64 , verbose = True, target_train=target_train):\n",
    "    input_layer = Input(shape=(784,))\n",
    "    layer = input_layer\n",
    "    #encode\n",
    "    for hidden_dim in autoencoders:\n",
    "        layer = Dense(hidden_dim, activation = 'relu')(layer)\n",
    "    autoencoders.pop(-1)\n",
    "    #decode\n",
    "    for hidden_dim in reversed(autoencoders):\n",
    "        layer = Dense(hidden_dim, activation = 'relu')(layer)\n",
    "    layer = Dense(784, activation = 'relu')(layer)\n",
    "    out = Dense(10, activation='sigmoid')(layer)    \n",
    "\n",
    "    target_train = to_categorical(target_train, num_classes=10)\n",
    "    model = Model(input_layer, out)\n",
    "    model.compile(optimizer = 'adadelta', loss = 'mean_squared_error')\n",
    "    model.fit(bin_train,target_train,\n",
    "                   epochs=iterations,\n",
    "                   batch_size=batch_size,\n",
    "                   shuffle=True,\n",
    "                   verbose=verbose)\n",
    "\n",
    "    predictions = np.argmax(model.predict(bin_test),axis=1)\n",
    "    print(metrics.classification_report(target_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.0963\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.0712\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.0428\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 0.0267\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 0.0190\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.0149\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 0.0121\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.0103\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 1s 116us/step - loss: 0.0090\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.0077\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 0.0068\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.0058\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.0051\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 1s 119us/step - loss: 0.0043\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 1s 122us/step - loss: 0.0039\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.0034\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 0.0029\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.0026\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.0023\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 1s 147us/step - loss: 0.0022\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 1s 143us/step - loss: 0.0019\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.0017\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 0.0015\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 0.0014\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 1s 112us/step - loss: 0.0013\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 1s 114us/step - loss: 0.0012\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.0012\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 0.0011\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 1s 146us/step - loss: 0.0011\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 0.0010\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 9.8197e-04\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 9.5576e-04\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 9.5672e-04\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 9.1567e-04\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 9.0795e-04\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 1s 126us/step - loss: 8.8456e-04\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 8.5879e-04\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 8.5117e-04\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 1s 142us/step - loss: 8.2373e-04\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 1s 141us/step - loss: 8.3316e-04\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 1s 135us/step - loss: 8.2347e-04\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 7.7878e-04\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 7.6446e-04\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 7.6061e-04\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 1s 136us/step - loss: 7.5714e-04\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 1s 120us/step - loss: 7.5021e-04\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 7.4537e-04\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 7.4219e-04\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 1s 118us/step - loss: 7.4031e-04\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 7.2921e-04\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.98      0.97       196\n",
      "        1.0       0.97      0.97      0.97       227\n",
      "        2.0       0.94      0.92      0.93       206\n",
      "        3.0       0.95      0.90      0.92       202\n",
      "        4.0       0.95      0.95      0.95       196\n",
      "        5.0       0.91      0.90      0.90       182\n",
      "        6.0       0.92      0.95      0.94       191\n",
      "        7.0       0.95      0.93      0.94       205\n",
      "        8.0       0.93      0.95      0.94       194\n",
      "        9.0       0.92      0.95      0.93       201\n",
      "\n",
      "avg / total       0.94      0.94      0.94      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_stacked_autoencoders(autoencoders = [150,100,50], iterations = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_MLP(layers, iterations = 50, batch_size = 64, verbose = True, target_train = target_train):\n",
    "    model = Sequential()\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "\n",
    "    target_train = to_categorical(target_train, num_classes=10)\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(bin_train,target_train,epochs=iterations,\n",
    "              batch_size=batch_size,verbose=verbose, shuffle=False)\n",
    "    predictions = np.argmax(model.predict(bin_test),axis=1)\n",
    "    print(metrics.classification_report(target_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.1032 - acc: 0.1771\n",
      "Epoch 2/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 0.0608 - acc: 0.5521\n",
      "Epoch 3/50\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 0.0364 - acc: 0.7674\n",
      "Epoch 4/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.0259 - acc: 0.8436\n",
      "Epoch 5/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.0212 - acc: 0.8705\n",
      "Epoch 6/50\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0180 - acc: 0.8932\n",
      "Epoch 7/50\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0159 - acc: 0.9036\n",
      "Epoch 8/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.0145 - acc: 0.9120\n",
      "Epoch 9/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.0132 - acc: 0.9195\n",
      "Epoch 10/50\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0122 - acc: 0.9250\n",
      "Epoch 11/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.0115 - acc: 0.9335\n",
      "Epoch 12/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.0101 - acc: 0.9379\n",
      "Epoch 13/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.0099 - acc: 0.9403\n",
      "Epoch 14/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.0091 - acc: 0.9450\n",
      "Epoch 15/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.0083 - acc: 0.9527\n",
      "Epoch 16/50\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.0081 - acc: 0.9513\n",
      "Epoch 17/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.0079 - acc: 0.9539\n",
      "Epoch 18/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.0079 - acc: 0.9511\n",
      "Epoch 19/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.0073 - acc: 0.9585\n",
      "Epoch 20/50\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0064 - acc: 0.9627\n",
      "Epoch 21/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 0.0069 - acc: 0.9573\n",
      "Epoch 22/50\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 0.0062 - acc: 0.9619\n",
      "Epoch 23/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 0.0063 - acc: 0.9619\n",
      "Epoch 24/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 0.0062 - acc: 0.9639\n",
      "Epoch 25/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.0056 - acc: 0.9673\n",
      "Epoch 26/50\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.0055 - acc: 0.9670\n",
      "Epoch 27/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 0.0054 - acc: 0.9689\n",
      "Epoch 28/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 0.0051 - acc: 0.9699\n",
      "Epoch 29/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 0.0051 - acc: 0.9698\n",
      "Epoch 30/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.0048 - acc: 0.9726\n",
      "Epoch 31/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.0046 - acc: 0.9726\n",
      "Epoch 32/50\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.0047 - acc: 0.9716\n",
      "Epoch 33/50\n",
      "8000/8000 [==============================] - 1s 89us/step - loss: 0.0045 - acc: 0.9723\n",
      "Epoch 34/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 0.0043 - acc: 0.9742\n",
      "Epoch 35/50\n",
      "8000/8000 [==============================] - 1s 80us/step - loss: 0.0043 - acc: 0.9748\n",
      "Epoch 36/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 0.0046 - acc: 0.9714\n",
      "Epoch 37/50\n",
      "8000/8000 [==============================] - 1s 76us/step - loss: 0.0043 - acc: 0.9741\n",
      "Epoch 38/50\n",
      "8000/8000 [==============================] - 1s 77us/step - loss: 0.0045 - acc: 0.9716\n",
      "Epoch 39/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.0043 - acc: 0.9739\n",
      "Epoch 40/50\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.0039 - acc: 0.9771\n",
      "Epoch 41/50\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.0042 - acc: 0.9740\n",
      "Epoch 42/50\n",
      "8000/8000 [==============================] - 1s 78us/step - loss: 0.0039 - acc: 0.9759\n",
      "Epoch 43/50\n",
      "8000/8000 [==============================] - 1s 79us/step - loss: 0.0042 - acc: 0.9738\n",
      "Epoch 44/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.0035 - acc: 0.9790\n",
      "Epoch 45/50\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.0042 - acc: 0.9738\n",
      "Epoch 46/50\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.0039 - acc: 0.9766\n",
      "Epoch 47/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.0038 - acc: 0.9765\n",
      "Epoch 48/50\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.0035 - acc: 0.9780\n",
      "Epoch 49/50\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.0035 - acc: 0.9779\n",
      "Epoch 50/50\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.0036 - acc: 0.9768\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.99      0.97       196\n",
      "        1.0       0.98      0.97      0.98       227\n",
      "        2.0       0.95      0.93      0.94       206\n",
      "        3.0       0.93      0.92      0.93       202\n",
      "        4.0       0.95      0.97      0.96       196\n",
      "        5.0       0.92      0.92      0.92       182\n",
      "        6.0       0.96      0.97      0.97       191\n",
      "        7.0       0.93      0.95      0.94       205\n",
      "        8.0       0.97      0.94      0.96       194\n",
      "        9.0       0.95      0.95      0.95       201\n",
      "\n",
      "avg / total       0.95      0.95      0.95      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://keras.io/getting-started/sequential-model-guide/\n",
    "layers = [\n",
    "    Dense(100,input_shape=(784,), activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(10, activation='sigmoid'),\n",
    "    ]\n",
    "\n",
    "report_MLP(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
